{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "header_font_size = 20\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"original_dataset/ReadBrownDwarf.mat\"\n",
    "data_path = \"original_dataset/\"\n",
    "idTE = np.load(data_path + \"idTE.npy\")\n",
    "idTR = np.load(data_path + \"idTR.npy\")\n",
    "labelTE = np.load(data_path + \"labelTE.npy\")\n",
    "labelTR = np.load(data_path + \"labelTR.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data from the mat file\n",
    "with h5py.File(mat_file, 'r') as f:\n",
    "    data = f[\"data\"]\n",
    "    data = pd.DataFrame(data).T\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation\n",
    "\n",
    "Adding $0$ scores as mean values of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data.replace(0, data.mean()), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(idTR)):\n",
    "    X_train_list.append(data.iloc[idTR[i] - 1]) #idTR is 1 indexed\n",
    "    X_test_list.append(data.iloc[idTE[i] - 1])  #idTE is 1 indexed\n",
    "    y_train_list.append(labelTR[i])\n",
    "    y_test_list.append(labelTE[i])\n",
    "\n",
    "X_train_arr = np.array(X_train_list)\n",
    "X_test_arr = np.array(X_test_list)\n",
    "y_train_arr = np.array(y_train_list)\n",
    "y_test_arr = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=13)\n",
    "\n",
    "#Reshaping to run PCA on the features\n",
    "X_train_arr_reshaped = X_train_arr.reshape(-1,26) \n",
    "X_test_arr_reshaped = X_test_arr.reshape(-1,26)\n",
    "print(X_train_arr_reshaped.shape)\n",
    "\n",
    "pca.fit(X_train_arr_reshaped)\n",
    "X_train_arr = pca.transform(X_train_arr_reshaped)\n",
    "X_test_arr = pca.transform(X_test_arr_reshaped)\n",
    "\n",
    "#Reshaping back to original shape\n",
    "X_train_arr = X_train_arr.reshape(5,4535,13)\n",
    "X_test_arr = X_test_arr.reshape(5,1134,13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network structure: $13$ x $10$ x $5$ x $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Convert numpy arrays or pandas DataFrames to PyTorch tensors if needed\n",
    "X_train_tens = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "y_train_tens = torch.tensor(y_train_arr, dtype=torch.long)\n",
    "\n",
    "# Defining a simple neural network class for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, activation=nn.Tanh()):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)# Output layer with single neuron (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Apply sigmoid activation for binary classification of final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED) # Added seed for reproducibility for all the activation functions\n",
    "losses = []\n",
    "for i in range(len(X_train_tens)):\n",
    "    # Initializing\n",
    "    input_size = X_train_tens[i].shape[1]\n",
    "    model = NeuralNet(input_size)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Prepare data for training using DataLoader\n",
    "    batch_size = 64\n",
    "    train_dataset = TensorDataset(X_train_tens[i], y_train_tens[i])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training the model\n",
    "    num_epochs = 15\n",
    "    temp_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(dim=1)  # Remove extra dimension for binary classification\n",
    "            loss = criterion(outputs, labels.float())  # Calculate loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss per epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "        temp_loss.append(running_loss / len(train_loader))\n",
    "    losses.append(temp_loss)\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_train_tens[i])\n",
    "        predictions = (predictions > 0.5).int()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "\n",
    "    dir_path = './models/'\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    #Saving model for later use\n",
    "    model_dir = './models/'\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    model_path = os.path.join(model_dir, f'fold{i}_binary_classification_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fold_colors = ['r', 'g', 'b', 'c', 'm']\n",
    "for i, loss in enumerate(losses):\n",
    "    plt.plot(loss, \"-o\", label=f\"fold_{i}\", color=fold_colors[i])\n",
    "plt.legend(fontsize = font_size - 2)\n",
    "plt.xlabel(\"Epoch\", size = font_size)\n",
    "plt.ylabel(\"Loss\", size = font_size)\n",
    "plt.title(\"Training Loss vs. Epoch for Different Activation Functions\", size = header_font_size - 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models/fold{i}_binary_classification_model.pth'))  # Load the trained model state (MACos)\n",
    "\n",
    "    X_train = X_train_tens[i].float()\n",
    "    y_train = y_train_tens[i].float()\n",
    "\n",
    "    # Prepare test dataset and dataloader\n",
    "    batch_size = 64\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    mcc_tr = matthews_corrcoef(y_true, y_pred)\n",
    "    mcc_scores.append(mcc_tr)\n",
    "\n",
    "    print(f\"Training: MCC-score: {mcc_tr}, check against the test set for overfitting\")\n",
    "\n",
    "print(f\"Mean MCC score on training set: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "all_metrics = {}\n",
    "all_confusion_matrices = {}\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size  \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models/fold{i}_binary_classification_model.pth'))  # Load the trained model state (MACos)\n",
    "\n",
    "\n",
    "    X_test = torch.tensor(X_test_arr[i], dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_arr[i], dtype=torch.float32)\n",
    "\n",
    "    # Prepare test dataset and dataloader\n",
    "    batch_size = 64\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculating different scoring methods\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred) # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary') \n",
    "    recall = recall_score(y_true, y_pred, average='binary')   \n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "    # Store the metrics in a dictionary for easy plotting\n",
    "    metrics = {'F1 Score': f1, 'MCC': mcc, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "    \n",
    "    all_metrics[f\"fold_{i}\"] = metrics\n",
    "    all_confusion_matrices[f\"fold_{i}\"] = conf_matrix\n",
    "\n",
    "    \n",
    "    print(f\"Testing MCC score: {metrics['MCC']}\")\n",
    "    mcc_scores.append(metrics['MCC'])\n",
    "\n",
    "    # Save the model\n",
    "    dir_path = './models/'\n",
    "    \n",
    "    # Checking if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    model_path = os.path.join(dir_path, f'fold{i}_binary_classification_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Mean MCC on testset: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_metrics)\n",
    "data = data.reset_index().rename(columns={'index': 'Metrics'})\n",
    "\n",
    "\n",
    "data = pd.melt(data, id_vars='Metrics', var_name='Activation Function', value_name='Value')\n",
    "\n",
    "data = data.set_index(['Metrics', 'Activation Function']).Value\n",
    "data.unstack().plot(kind='bar', stacked=False, color = fold_colors, fontsize = font_size-3)\n",
    "plt.legend(fontsize = font_size-2)\n",
    "plt.title(\"Metrics comparision over different folds\", size = header_font_size)\n",
    "plt.ylim(0.8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for i in range(len(X_train_tens)):\n",
    "    conf_matrix = all_confusion_matrices[f\"fold_{i}\"]\n",
    "\n",
    "    # Create a heatmap for the confusion matrix for the i-th fold\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar = False, annot_kws={\"size\": font_size})\n",
    "\n",
    "    axes[i].set_title(f'Confusion Matrix (fold_{i})', size = header_font_size)\n",
    "    axes[i].set_xlabel('Predicted', size = font_size)\n",
    "    axes[i].set_ylabel('True', size = font_size)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
