{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "#Hyperparameters\n",
    "num_epochs_global = 33\n",
    "batch_size_global = 32\n",
    "num_neurons_1hl = 14\n",
    "num_neurons_2hl = 10\n",
    "\n",
    "\n",
    "\n",
    "#Plotting\n",
    "font_size = 16\n",
    "header_font_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"original_data/ReadBrownDwarf.mat\"\n",
    "data_path = \"original_data/\"\n",
    "idTE = np.load(data_path + \"idTE.npy\")\n",
    "idTR = np.load(data_path + \"idTR.npy\")\n",
    "labelTE = np.load(data_path + \"labelTE.npy\")\n",
    "labelTR = np.load(data_path + \"labelTR.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data from the mat file\n",
    "with h5py.File(mat_file, 'r') as f:\n",
    "    data = f[\"data\"]\n",
    "    data = pd.DataFrame(data).T\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of replacing zero-entries with the mean of the column, we train a linear regression model on the rows with complete data and make the model predict on our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "np.random.seed(SEED)\n",
    "columns_with_zero = data.columns[(data == 0).any()].tolist()\n",
    "\n",
    "print(columns_with_zero)\n",
    "target_columns = columns_with_zero\n",
    "\n",
    "for target_column in target_columns:\n",
    "    df_zeros = data[data[target_column] == 0]\n",
    "    df_no_zeros = data[data[target_column] != 0]\n",
    "\n",
    "    X_train = df_no_zeros.drop(columns=target_column)\n",
    "    y_train = df_no_zeros[target_column]\n",
    "\n",
    "    X_test = df_zeros.drop(columns=target_column)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    # Replace the zero values in the original dataframe\n",
    "    data.loc[data[target_column] == 0, target_column] = y_test\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have imputed the missing values using a ML model we want to scale the data as we do not know what the variance of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and test data before scaling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Splitting in training and test data\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(idTR)):\n",
    "    X_train_list.append(data.iloc[idTR[i] - 1]) #idTR is 1 indexed\n",
    "    X_test_list.append(data.iloc[idTE[i] - 1])  #idTE is 1 indexed\n",
    "    y_train_list.append(labelTR[i])\n",
    "    y_test_list.append(labelTE[i])\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_list[i] = scaler.fit_transform(X_train_list[i])\n",
    "    X_test_list[i] = scaler.transform(X_test_list[i])\n",
    "\n",
    "X_train_arr = np.array(X_train_list)\n",
    "X_test_arr = np.array(X_test_list)\n",
    "y_train_arr = np.array(y_train_list)\n",
    "y_test_arr = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network structure: $26$ x $14$ x $10$ x $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED) # Added seed for reproducibility for all the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Convert numpy arrays or pandas DataFrames to PyTorch tensors if needed\n",
    "X_train_tens = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "y_train_tens = torch.tensor(y_train_arr, dtype=torch.long)\n",
    "\n",
    "# Defining a simple neural network class for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, activation=nn.ReLU()):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_neurons_1hl)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(num_neurons_1hl, num_neurons_2hl)\n",
    "        self.fc3 = nn.Linear(num_neurons_2hl, 1)# Output layer with single neuron (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Apply sigmoid activation for binary classification of final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "losses_fold = []\n",
    "\n",
    "\n",
    "val_losses = []  # Store validation losses\n",
    "all_losses = []  # Store training losses for all folds\n",
    "all_val_losses = []  # Store validation losses for all folds\n",
    "all_minimas = [] #Store lowest validation losses for all folds\n",
    "\n",
    "for i in range(len(X_train_tens)):\n",
    "    # Initializing\n",
    "    input_size = X_train_tens[i].shape[1]\n",
    "    model = NeuralNet(input_size)\n",
    "\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Prepare data for training using DataLoader\n",
    "    batch_size = batch_size_global\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_tens[i], y_train_tens[i], test_size=0.2)  # Split data into training and validation sets\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)  # Create validation dataset\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # Create validation DataLoader\n",
    "\n",
    "    # Training loop\n",
    "    # Training loop\n",
    "    temp_loss = []\n",
    "    num_epochs = num_epochs_global\n",
    "    losses = []  # Store training losses for this fold\n",
    "    val_losses = []  # Store validation losses for this fold\n",
    "    minima_index = 0\n",
    "    minima = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(dim=1)  # Remove extra dimension for binary classification\n",
    "            loss = criterion(outputs, labels.float())  # Calculate loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss per epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "        temp_loss.append(running_loss / len(train_loader))\n",
    "    \n",
    "        # Calculate validation loss\n",
    "        val_loss = 0.0\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze(dim=1)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        model.train()  # Switch back to training mode\n",
    "\n",
    "        # Store losses\n",
    "        losses.append(running_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        if val_loss < minima:\n",
    "            minima_index = epoch\n",
    "            minima = val_loss\n",
    "\n",
    "        # Print average loss per epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    losses_fold.append(temp_loss)\n",
    "    all_losses.append(losses)\n",
    "    all_val_losses.append(val_losses)\n",
    "    all_minimas.append(minima_index)\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_train_tens[i])\n",
    "        predictions = (predictions > 0.5).int()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "\n",
    "    dir_path = './models/'\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    #Saving model for later use\n",
    "    model_dir = './models/'\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    model_path = os.path.join(model_dir, f'fold{i}_binary_classification_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    # Calculate mean losses\n",
    "mean_losses = np.array([sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*all_losses)])\n",
    "mean_val_losses = np.array([sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*all_val_losses)])\n",
    "print(mean_losses)\n",
    "print(mean_val_losses)\n",
    "for epoch in range(num_epochs_global):\n",
    "    print(f\"Epoch {epoch+1}, Mean Training Loss: {mean_losses[epoch]}, Mean Validation Loss: {mean_val_losses[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_fold = np.array(losses_fold)\n",
    "losses_fold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses.shape, mean_val_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate mean losses\n",
    "#mean_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*all_losses)]\n",
    "#mean_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*all_val_losses)]\n",
    "minima = np.min(mean_val_losses)\n",
    "minima_index = np.argmin(mean_val_losses)\n",
    "\n",
    "plt.plot(range(1, num_epochs+1), mean_losses, \"-o\", label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), mean_val_losses, \"-o\", label='Validation Loss')\n",
    "plt.plot(minima_index, minima, \"-o\", color = \"r\", label = \"Minima\")\n",
    "\n",
    "plt.annotate(f\"({minima_index}, {format(minima, '.3f')})\", (minima_index-4, 0.1))\n",
    "plt.title('Mean Training and Validation Losses Across All Folds', fontsize = header_font_size-5)\n",
    "plt.xlabel('Epochs', fontsize = font_size)\n",
    "plt.ylabel('Loss', fontsize = font_size )\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fold_colors = ['r', 'g', 'b', 'c', 'm']\n",
    "for i, loss in enumerate(losses_fold):\n",
    "    plt.plot(loss, \"-o\", label=f\"fold_{i}\", color=fold_colors[i % len(fold_colors)])\n",
    "plt.legend(fontsize = font_size - 2)\n",
    "plt.xlabel(\"Epoch\", size = font_size)\n",
    "plt.ylabel(\"Loss\", size = font_size)\n",
    "plt.title(\"Training Loss vs. Epoch for Different Activation Functions\", size = header_font_size - 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models/fold{i}_binary_classification_model.pth'))  # Load the trained model state (MACos)\n",
    "\n",
    "    X_train = X_train_tens[i].float()\n",
    "    y_train = y_train_tens[i].float()\n",
    "\n",
    "    # Prepare test dataset and dataloader\n",
    "    batch_size = batch_size_global\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    mcc_tr = matthews_corrcoef(y_true, y_pred)\n",
    "    mcc_scores.append(mcc_tr)\n",
    "\n",
    "    print(f\"Training: MCC-score: {mcc_tr}, check against the test set for overfitting\")\n",
    "\n",
    "print(f\"Mean MCC score on training set: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "all_metrics = {}\n",
    "all_confusion_matrices = {}\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size  \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models/fold{i}_binary_classification_model.pth'))  # Load the trained model state (MACos)\n",
    "\n",
    "\n",
    "    X_test = torch.tensor(X_test_arr[i], dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_arr[i], dtype=torch.float32)\n",
    "\n",
    "    # Prepare test dataset and dataloader\n",
    "    batch_size = batch_size_global\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculating different scoring methods\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred) # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary') \n",
    "    recall = recall_score(y_true, y_pred, average='binary')   \n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "    # Store the metrics in a dictionary for easy plotting\n",
    "    metrics = {'F1 Score': f1, 'MCC': mcc, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "    \n",
    "    all_metrics[f\"fold_{i}\"] = metrics\n",
    "    all_confusion_matrices[f\"fold_{i}\"] = conf_matrix\n",
    "\n",
    "    \n",
    "    print(f\"Testing MCC score: {metrics['MCC']}\")\n",
    "    mcc_scores.append(metrics['MCC'])\n",
    "\n",
    "print(f\"Mean MCC on entire TEST SET: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_metrics)\n",
    "data = data.reset_index().rename(columns={'index': 'Metrics'})\n",
    "\n",
    "\n",
    "data = pd.melt(data, id_vars='Metrics', var_name='Fold', value_name='Value')\n",
    "\n",
    "data = data.set_index(['Metrics', 'Fold']).Value\n",
    "data.unstack().plot(kind='bar', stacked=False, color = fold_colors, fontsize = font_size-3)\n",
    "plt.legend(fontsize = font_size-2)\n",
    "plt.title(\"Metrics comparison over different folds\", size = header_font_size)\n",
    "plt.ylim(0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the 3rd and 4th quartile since they are deviating a lot from the mean\n",
    "mean_Accuracy = np.mean(data['Accuracy'][:3])\n",
    "mean_F1_score = np.mean(data['F1 Score'][:3])\n",
    "mean_MCC      = np.mean(data['MCC'][:3])\n",
    "mean_Precision= np.mean(data['Precision'][:3])\n",
    "mean_Recall   = np.mean(data['Recall'][:3])\n",
    "\n",
    "Mean_metrics = [mean_Accuracy, mean_F1_score, mean_MCC, mean_Precision, mean_Recall]\n",
    "names = [\"Accuracy\", \"F1 Score\", \"MCC\", \"Precision\", \"Recall\"]\n",
    "\n",
    "# Mean_metrics.unstack().plot(kind='bar', stacked=False, color = fold_colors, fontsize = font_size-3)\n",
    "plt.bar(names, Mean_metrics)\n",
    "plt.title(\"Mean Metrics\", size = header_font_size)\n",
    "plt.ylim(0.9,1)\n",
    "\n",
    "\n",
    "print(\"Mean Metrics for interquartile range:\")\n",
    "for i,elem in enumerate(Mean_metrics):\n",
    "    \n",
    "    print(f\"{names[i]}: {elem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for i in range(len(X_train_tens)):\n",
    "    conf_matrix = all_confusion_matrices[f\"fold_{i}\"]\n",
    "\n",
    "    # Create a heatmap for the confusion matrix for the i-th fold\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar = False, annot_kws={\"size\": font_size})\n",
    "\n",
    "    axes[i].set_title(f'Confusion Matrix (fold_{i})', size = header_font_size)\n",
    "    axes[i].set_xlabel('Predicted', size = font_size)\n",
    "    axes[i].set_ylabel('True', size = font_size)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_conf_matrix = np.zeros((2,2))\n",
    "\n",
    "for fold, conf_values in all_confusion_matrices.items():\n",
    "    print(fold)\n",
    "    if (fold != \"fold_3\" and fold !=\"fold_4\"):\n",
    "        for i in range(len(conf_values)):\n",
    "            for j in range(len(conf_values[i])):\n",
    "                temp_conf_matrix[i][j] += conf_values[i][j]\n",
    "average_conf_matrix = temp_conf_matrix/3\n",
    "\n",
    "\n",
    "sns.heatmap(average_conf_matrix, annot=True, fmt='.1f', cmap='Blues', cbar = False, annot_kws={\"size\": font_size})\n",
    "\n",
    "plt.title(f'Mean Confusion Matrix', size = header_font_size)\n",
    "plt.xlabel('Predicted', size = font_size)\n",
    "plt.ylabel('True', size = font_size)\n",
    "plt.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "all_confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
