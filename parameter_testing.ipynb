{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the actual dataset from the matlab file given instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"original_dataset/ReadBrownDwarf.mat\"\n",
    "data_path = \"original_dataset/\"\n",
    "idTE = np.load(data_path + \"idTE.npy\")\n",
    "idTR = np.load(data_path + \"idTR.npy\")\n",
    "labelTE = np.load(data_path + \"labelTE.npy\")\n",
    "labelTR = np.load(data_path + \"labelTR.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data from the mat file\n",
    "with h5py.File(mat_file, 'r') as f:\n",
    "    data = f[\"data\"]\n",
    "    data = pd.DataFrame(data).T\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing values with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Check which columns have zero values\n",
    "columns_with_zero = data.columns[(data == 0).any()].tolist()\n",
    "\n",
    "print(columns_with_zero)\n",
    "target_columns = columns_with_zero\n",
    "\n",
    "for target_column in target_columns:\n",
    "    df_zeros = data[data[target_column] == 0]\n",
    "    df_no_zeros = data[data[target_column] != 0]\n",
    "\n",
    "    X_train = df_no_zeros.drop(columns=target_column)\n",
    "    y_train = df_no_zeros[target_column]\n",
    "\n",
    "    X_test = df_zeros.drop(columns=target_column)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    # Replace the zero values in the original dataframe\n",
    "    data.loc[data[target_column] == 0, target_column] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data= pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(idTR)):\n",
    "    X_train_list.append(data.iloc[idTR[i] - 1]) #idTR is 1 indexed\n",
    "    X_test_list.append(data.iloc[idTE[i] - 1])  #idTE is 1 indexed\n",
    "    y_train_list.append(labelTR[i])\n",
    "    y_test_list.append(labelTE[i])\n",
    "\n",
    "X_train_arr = np.array(X_train_list)\n",
    "X_test_arr = np.array(X_test_list)\n",
    "y_train_arr = np.array(y_train_list)\n",
    "y_test_arr = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_4_test = idTE[3]\n",
    "for i in range(4):\n",
    "    if i != 3:\n",
    "        common_elements = set(fold_4_test) & set(idTR[i])\n",
    "        num_common_elements = len(common_elements)\n",
    "        print(f\"The two lists have {num_common_elements} common elements: {common_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "BATCHSIZE = 32\n",
    "OUTPUT_NODES = 1\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 15\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 26\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 8, 16)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        #p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        #layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, OUTPUT_NODES))\n",
    "    layers.append(nn.Sigmoid())\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_valid(X, y, test_size=0.2, BATCHSIZE=32):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def objective(trial):\n",
    "    \n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"Adagrad\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters())\n",
    "\n",
    "    train_loader, valid_loader = get_train_valid(X_train_arr, y_train_arr)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    results = {}\n",
    "\n",
    " \n",
    "    for fold in range(5):\n",
    "        network = define_model(trial).to(DEVICE)\n",
    "        network.train()\n",
    "\n",
    "        # Training of the model.\n",
    "        for epoch in range(EPOCHS):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # Limiting training data for faster epochs.\n",
    "                if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                    break\n",
    "\n",
    "                #data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data).squeeze()\n",
    "\n",
    "                \n",
    "            \n",
    "                output =  output.float()\n",
    "                loss_fn = nn.BCELoss()\n",
    "               \n",
    "                loss = loss_fn(output, target.float())\n",
    "            \n",
    "                loss.backward()\n",
    "          \n",
    "                optimizer.step()\n",
    "             \n",
    "\n",
    "            # Validation of the model.\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                    # Limiting validation data.\n",
    "                    if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                        break\n",
    "                \n",
    "                    data, target = data.to(DEVICE), target.to(DEVICE)                \n",
    "                    output = model(data).squeeze()\n",
    "                    output =  output.float()\n",
    "                    \n",
    "                    # Get the index of the max log-probability.\n",
    "                    pred = (output < 0.5)\n",
    "\n",
    "        \n",
    "            score = matthews_corrcoef(target.T, pred)\n",
    "\n",
    "            trial.report(score, epoch)\n",
    "\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        results[fold] = score\n",
    "\n",
    "    return sum(results.values()) / len(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_global = EPOCHS\n",
    "batch_size_global = BATCHSIZE\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Convert numpy arrays or pandas DataFrames to PyTorch tensors if needed\n",
    "X_train_tens = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "y_train_tens = torch.tensor(y_train_arr, dtype=torch.long)\n",
    "\n",
    "# Defining a simple neural network class for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, activation=nn.ReLU()):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(input_size, 11)\n",
    "        self.fc2 = nn.Linear(11, 11)\n",
    "        self.fc3 = nn.Linear(11, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [] #Might turn into dict to make it more readable\n",
    "for i in range(len(X_train_tens)):\n",
    "    input_size = X_train_tens[i].shape[1]\n",
    "    model = NeuralNet(input_size)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr =0.0017748712435351215)\n",
    "\n",
    "    # Prepare data for training using DataLoader\n",
    "    batch_size = batch_size_global\n",
    "    train_dataset = TensorDataset(X_train_tens[i], y_train_tens[i])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = num_epochs_global\n",
    "    temp_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(dim=1) \n",
    "            loss = criterion(outputs, labels.float())  # Calculate loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        temp_loss.append(running_loss / len(train_loader))\n",
    "    losses.append(temp_loss)\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_train_tens[i])\n",
    "        predictions = (predictions > 0.5).int()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "   \n",
    "    #Saving model for later use\n",
    "    model_dir = './models_experimental/'\n",
    "    os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    model_path = os.path.join(model_dir, f'fold{i}_binary_classification_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i, loss in enumerate(losses):\n",
    "    plt.plot(loss, \"-o\", label=f\"fold_{i}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs. Epoch for Different Activation Functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size  \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models_experimental/fold{i}_binary_classification_model.pth'))  # Load the trained model state\n",
    "\n",
    "    X_train = X_train_tens[i].float()\n",
    "    y_train = y_train_tens[i].float()\n",
    "\n",
    "    batch_size = batch_size_global\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    mcc_tr = matthews_corrcoef(y_true, y_pred)\n",
    "    mcc_scores.append(mcc_tr)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Training: MCC-score: {mcc_tr}, check against the test set for overfitting\")\n",
    "\n",
    "print(f\"Mean MCC score on training set: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "mcc_scores = []\n",
    "all_metrics = {}\n",
    "all_confusion_matrices = {}\n",
    "for i in range(len(X_train_tens)):\n",
    "    print(f\"Evaluating model on fold: {i}\")\n",
    "    # Load the trained model\n",
    "    input_size = input_size  \n",
    "    model = NeuralNet(input_size)\n",
    "    model.load_state_dict(torch.load(f'models_experimental/fold{i}_binary_classification_model.pth'))  # Load the trained model state \n",
    "\n",
    "    X_test = torch.tensor(X_test_arr[i], dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_arr[i], dtype=torch.float32)\n",
    "\n",
    "    # Prepare test dataset and dataloader\n",
    "    batch_size = batch_size_global\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "    model.eval()  \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    #print(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    #print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    #print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
    "\n",
    "    # Compute accuracy, precision and recall\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary')  # Change average as needed\n",
    "    recall = recall_score(y_true, y_pred, average='binary')  # Change average as needed\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "    # Store the metrics in a dictionary for easy plotting\n",
    "    metrics = {'F1 Score': f1, 'MCC': mcc, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "    \n",
    "    all_metrics[f\"fold_{i}\"] = metrics\n",
    "    all_confusion_matrices[f\"fold_{i}\"] = conf_matrix\n",
    "\n",
    "    \n",
    "    print(f\"Testing: {metrics['MCC']}\")\n",
    "    mcc_scores.append(metrics['MCC'])\n",
    "\n",
    "    dir_path = './models/'\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(dir_path, f'fold{i}_binary_classification_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Mean MCC on testset: {np.mean(mcc_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the all_metrics dictionary to a DataFrame\n",
    "data = pd.DataFrame(all_metrics)\n",
    "data = data.reset_index().rename(columns={'index': 'Metrics'})\n",
    "\n",
    "\n",
    "data = pd.melt(data, id_vars='Metrics', var_name='Activation Function', value_name='Value')\n",
    "\n",
    "data = data.set_index(['Metrics', 'Activation Function']).Value\n",
    "colors = [\"orange\", \"red\", \"blue\", \"green\"]\n",
    "data.unstack().plot(kind='bar', stacked=False, color = colors)\n",
    "plt.ylim(0.6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test[y_test == 0]))\n",
    "print(len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for i in range(len(X_train_tens)):\n",
    "    conf_matrix = all_confusion_matrices[f\"fold_{i}\"]\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "\n",
    "    axes[i].set_title(f'Confusion Matrix (fold_{i})')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
