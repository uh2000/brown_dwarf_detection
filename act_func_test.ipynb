{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "header_font_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the actual dataset from the matlab file given instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"original_dataset/ReadBrownDwarf.mat\"\n",
    "data_path = \"original_dataset/\"\n",
    "idTE = np.load(data_path + \"idTE.npy\")\n",
    "idTR = np.load(data_path + \"idTR.npy\")\n",
    "labelTE = np.load(data_path + \"labelTE.npy\")\n",
    "labelTR = np.load(data_path + \"labelTR.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data from the mat file\n",
    "with h5py.File(mat_file, 'r') as f:\n",
    "    data = f[\"data\"]\n",
    "    data = pd.DataFrame(data).T\n",
    "data = data.replace(0, data.mean())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(idTR)):\n",
    "    X_train_list.append(data.iloc[idTR[i] - 1]) #idTR is 1 indexed\n",
    "    X_test_list.append(data.iloc[idTE[i] - 1])  #idTE is 1 indexed\n",
    "    y_train_list.append(labelTR[i])\n",
    "    y_test_list.append(labelTE[i])\n",
    "\n",
    "X_train_arr = np.array(X_train_list)\n",
    "X_test_arr = np.array(X_test_list)\n",
    "y_train_arr = np.array(y_train_list)\n",
    "y_test_arr = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added scaling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_arr_reshaped = X_train_arr.reshape(-1,26) #mixing all folds together before applying PCA\n",
    "X_test_arr_reshaped = X_test_arr.reshape(-1,26)\n",
    "\n",
    "X_train_arr_reshaped = scaler.fit_transform(X_train_arr_reshaped)\n",
    "X_test_arr_reshaped = scaler.transform(X_test_arr_reshaped)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=13)\n",
    "X_train_arr_reshaped = X_train_arr.reshape(-1,26) #mixing all folds together before applying PCA\n",
    "X_test_arr_reshaped = X_test_arr.reshape(-1,26)\n",
    "\n",
    "pca.fit(X_train_arr_reshaped)\n",
    "X_train_arr = pca.transform(X_train_arr_reshaped)\n",
    "X_test_arr = pca.transform(X_test_arr_reshaped)\n",
    "X_train_arr = X_train_arr.reshape(5,4535,13) #reshaping back to original shape\n",
    "X_test_arr = X_test_arr.reshape(5,1134,13) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net for binary classification 5x32x64x32x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays or pandas DataFrames to PyTorch tensors if needed\n",
    "X_train_tens = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "y_train_tens = torch.tensor(y_train_arr, dtype=torch.long)\n",
    "\n",
    "# Defining a simple neural network class for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, activation):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.activation = activation       \n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)# Output layer with single neuron (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Apply sigmoid activation for binary classification of final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED) # Added seed for reproducibility for all the activation functions\n",
    "act_losses = [] \n",
    "activation_functions = [nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Hardtanh()]\n",
    "act_colors = {\"ReLU\": \"red\", \"Sigmoid\": \"blue\", \"Tanh\": \"green\", \"Hardtanh\": \"orange\"}\n",
    "for act in activation_functions:\n",
    "    losses = []\n",
    "    for i in range(len(X_train_tens)):\n",
    "        # Initialize the neural network\n",
    "        input_size = X_train_tens[i].shape[1]\n",
    "        model = NeuralNet(input_size, act)\n",
    "    \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # Prepare data for training using DataLoader\n",
    "        batch_size = 64\n",
    "        train_dataset = TensorDataset(X_train_tens[i], y_train_tens[i])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = 15\n",
    "        temp_loss = []\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze(dim=1)  # Remove extra dimension for binary classification\n",
    "                loss = criterion(outputs, labels.float())  # Calculate loss\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "            temp_loss.append(running_loss / len(train_loader))\n",
    "        losses.append(temp_loss)\n",
    "\n",
    "        # Assuming X_test is your test data\n",
    "        model.eval()  #Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_train_tens[i])\n",
    "            predictions = (predictions > 0.5).int()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "\n",
    "        model_dir = f'./models/activation_test/{type(act).__name__}/'\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        #Saving the model for later use\n",
    "        model_path = os.path.join(model_dir, f'fold{i}_binary_classification_model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    act_losses.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "act_mean_losses = np.mean(act_losses, axis=1) #Mean loss for each activation function\n",
    "\n",
    "for i, loss in enumerate(act_mean_losses):\n",
    "    name_act = type(activation_functions[i]).__name__\n",
    "    c = act_colors[name_act]\n",
    "    plt.plot(loss, \"-o\", label=name_act, color = c)\n",
    "plt.legend(fontsize = font_size-2)\n",
    "plt.xlabel(\"Epoch\", size = font_size)\n",
    "plt.ylabel(\"Loss\", size = font_size)\n",
    "plt.title(\"Training Loss vs. Epoch for Different Activation Functions\", size = font_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing last epoch on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "for act in activation_functions:\n",
    "    print(f\"Activation function: {type(act).__name__}\")\n",
    "    for i in range(len(X_train_tens)):\n",
    "        print(f\"Evaluating model on fold: {i}\")\n",
    "        # Load the trained model\n",
    "        input_size = input_size  \n",
    "        model = NeuralNet(input_size, act)\n",
    "        model.load_state_dict(torch.load(f'models/activation_test/{type(act).__name__}/fold{i}_binary_classification_model.pth')) \n",
    "\n",
    "\n",
    "        X_train = X_train_tens[i].float()\n",
    "        y_train = y_train_tens[i].float()\n",
    "\n",
    "        #Prepare test dataset and dataloader\n",
    "        batch_size = 64\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  #No need to shuffle for testing\n",
    "\n",
    "        model.eval()  #Switch to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad(): #No need for gradient on test run\n",
    "            for inputs, labels in train_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate Matthews Correlation Coefficient (MCC)\n",
    "        mcc_tr = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "        print(f\"Training: MCC-score: {mcc_tr}, check against the test set for overfitting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "act_all_metrics = {}\n",
    "act_mean_metrics = {}\n",
    "act_all_confusion_matrices = {}\n",
    "act_mean_confusion_matrices = {}\n",
    "\n",
    "for act in activation_functions:\n",
    "    all_metrics = {}\n",
    "    all_confusion_matrices = {}\n",
    "    act_name = type(act).__name__\n",
    "    print(f\"Activation function: {act_name}\")\n",
    "    print(\"---------------------------------\")\n",
    "    temp_F1, temp_MCC, temp_acc, temp_prec, temp_rec = 0,0,0,0,0\n",
    "    for i in range(len(X_train_tens)):\n",
    "        print(f\"    Evaluating model on fold: {i}\")\n",
    "        # Load the trained model\n",
    "        input_size = input_size  # Assuming the input size based on X_train\n",
    "        model = NeuralNet(input_size, act)\n",
    "        model.load_state_dict(torch.load(f'models/activation_test/{act_name}/fold{i}_binary_classification_model.pth'))  # Load the trained model state \n",
    "\n",
    "        X_test = torch.tensor(X_test_arr[i], dtype=torch.float32)\n",
    "        y_test = torch.tensor(y_test_arr[i], dtype=torch.float32)\n",
    "\n",
    "        #Prepare test dataset and dataloader\n",
    "        batch_size = 64\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  #No need to shuffle for testing\n",
    "\n",
    "        model.eval()  #Switch to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad(): #No need to calculate gradients in test run\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        #Calculate Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "        #Computes accuracy, precision, recall and conf. matrix\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='binary') #Binary average, since we have a binary classification problem\n",
    "        recall = recall_score(y_true, y_pred, average='binary')  \n",
    "        conf_matrix = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "        #Stores the metrics in a dictionary for easy plotting\n",
    "        metrics = {'F1 Score': f1, 'MCC': mcc, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "        temp_F1 += f1\n",
    "        temp_MCC += mcc\n",
    "        temp_acc += accuracy\n",
    "        temp_prec += precision\n",
    "        temp_rec += recall\n",
    "        temp_MCC += mcc\n",
    "        all_metrics[f\"fold_{i}\"] = metrics\n",
    "        all_confusion_matrices[f\"fold_{i}\"] = conf_matrix\n",
    "        print(f\"Testing: {metrics['MCC']}\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"\\n\")\n",
    "    #Saving the metrics and confusion matrices with respective act. func. for later use\n",
    "    act_all_metrics[act_name] = all_metrics\n",
    "    act_all_confusion_matrices[act_name] = all_confusion_matrices\n",
    "    act_mean_metrics[act_name] = {'F1 Score': temp_F1/len(X_train_tens), 'MCC': temp_MCC/len(X_train_tens), 'Accuracy': temp_acc/len(X_train_tens), 'Precision': temp_prec/len(X_train_tens), 'Recall': temp_rec/len(X_train_tens)}\n",
    "    act_mean_confusion_matrices[act_name] = temp_MCC/len(X_train_tens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU\n",
    "\n",
    "Evaluating model on fold: 0\n",
    "Testing: 0.9673819604992024\n",
    "\n",
    "Evaluating model on fold: 1\n",
    "Testing: 0.9760993833288099\n",
    "\n",
    "Evaluating model on fold: 2\n",
    "Testing: 0.9607060027297806\n",
    "\n",
    "Evaluating model on fold: 3\n",
    "Testing: 0.9738386237588376\n",
    "\n",
    "Evaluating model on fold: 4\n",
    "Testing: 0.9546147643325661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_metric = pd.DataFrame(act_mean_metrics)\n",
    "#Reset the index to make 'Metrics' a column\n",
    "data_metric = data_metric.reset_index().rename(columns={'index': 'Metrics'})\n",
    "\n",
    "\n",
    "data_metric = pd.melt(data_metric, id_vars='Metrics', var_name='Activation Function', value_name='Value')\n",
    "\n",
    "data_metric = data_metric.set_index(['Metrics', 'Activation Function']).Value\n",
    "colors = [\"orange\", \"red\", \"blue\", \"green\"]\n",
    "data_metric.unstack().plot(kind='bar', stacked=False, color = colors)\n",
    "plt.ylim(0.8,1)\n",
    "plt.title(\"Average Metrics for Different Activation Functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mean_confusion_matrices\n",
    "# act_all_confusion_matrices\n",
    "act_all_confusion_matrices\n",
    "\n",
    "mean_confusion_matrices = {}\n",
    "for act_func, matrices in act_all_confusion_matrices.items():\n",
    "    stacked_matrices = np.dstack(list(matrices.values()))\n",
    "    mean_matrix = np.mean(stacked_matrices, axis=2, dtype=int)\n",
    "    mean_confusion_matrices[act_func] = mean_matrix\n",
    "\n",
    "print(mean_confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, len(activation_functions), figsize=(20, 5))\n",
    "\n",
    "for i, act in enumerate(activation_functions):\n",
    "    act_name = type(act).__name__\n",
    "    conf_matrix = mean_confusion_matrices[act_name]\n",
    "\n",
    "    #Creates a heatmap for the confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar=False, annot_kws={\"size\": font_size})\n",
    "\n",
    "    axes[i].set_title(f'{act_name}', size = header_font_size)\n",
    "    axes[i].set_xlabel('Predicted', size = font_size)\n",
    "    axes[i].set_ylabel('True', size = font_size)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "fig.suptitle(\"Average Confusion Matrices for Different Activation Functions\", fontsize=header_font_size + 5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
