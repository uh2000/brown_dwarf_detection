{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 16\n",
    "header_font_size = 20\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = \"original_data/ReadBrownDwarf.mat\"\n",
    "data_path = \"original_data/\"\n",
    "idTE = np.load(data_path + \"idTE.npy\")\n",
    "idTR = np.load(data_path + \"idTR.npy\")\n",
    "labelTE = np.load(data_path + \"labelTE.npy\")\n",
    "labelTR = np.load(data_path + \"labelTR.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>326.808170</td>\n",
       "      <td>2.686124</td>\n",
       "      <td>5.5560</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.7600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.9470</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0720</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.044</td>\n",
       "      <td>5.562</td>\n",
       "      <td>5.539</td>\n",
       "      <td>5.479</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.200440</td>\n",
       "      <td>25.880459</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.6116</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.5167</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.059</td>\n",
       "      <td>6.896</td>\n",
       "      <td>6.921</td>\n",
       "      <td>6.932</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144.038450</td>\n",
       "      <td>-12.459262</td>\n",
       "      <td>10.5120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.6920</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.8660</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.9890</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.408</td>\n",
       "      <td>10.400</td>\n",
       "      <td>10.378</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209.662170</td>\n",
       "      <td>21.696203</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.8603</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.037</td>\n",
       "      <td>5.671</td>\n",
       "      <td>5.717</td>\n",
       "      <td>5.704</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.162550</td>\n",
       "      <td>-20.639620</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.3206</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.9504</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.036</td>\n",
       "      <td>5.430</td>\n",
       "      <td>5.402</td>\n",
       "      <td>5.333</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>13.245956</td>\n",
       "      <td>49.443574</td>\n",
       "      <td>9.1227</td>\n",
       "      <td>0.2347</td>\n",
       "      <td>9.1823</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0066</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.144</td>\n",
       "      <td>8.358</td>\n",
       "      <td>8.389</td>\n",
       "      <td>8.398</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>29.647854</td>\n",
       "      <td>48.432832</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.5538</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.7253</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.297</td>\n",
       "      <td>8.778</td>\n",
       "      <td>8.804</td>\n",
       "      <td>8.766</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>346.990957</td>\n",
       "      <td>54.326940</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.8405</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>9.2025</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>10.1089</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.327</td>\n",
       "      <td>9.180</td>\n",
       "      <td>9.187</td>\n",
       "      <td>9.180</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>331.173131</td>\n",
       "      <td>46.427253</td>\n",
       "      <td>10.4915</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>10.2538</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>10.0057</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>10.4418</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.693</td>\n",
       "      <td>9.678</td>\n",
       "      <td>9.622</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>321.694044</td>\n",
       "      <td>43.812346</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.8477</td>\n",
       "      <td>0.3777</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.1665</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.115</td>\n",
       "      <td>7.248</td>\n",
       "      <td>7.309</td>\n",
       "      <td>7.281</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5669 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1        2       3        4       5        6   \\\n",
       "0     326.808170   2.686124   5.5560  0.0000   5.7600  0.0000   5.9470   \n",
       "1     321.200440  25.880459   0.0000  0.0000   7.6116  0.0010   0.0000   \n",
       "2     144.038450 -12.459262  10.5120  0.0000  10.6920  0.0000  10.8660   \n",
       "3     209.662170  21.696203   0.0000  0.0000   0.0000  0.0000   6.8603   \n",
       "4      65.162550 -20.639620   0.0000  0.0000   0.0000  0.0000   6.3206   \n",
       "...          ...        ...      ...     ...      ...     ...      ...   \n",
       "5664   13.245956  49.443574   9.1227  0.2347   9.1823  0.0010   0.0000   \n",
       "5665   29.647854  48.432832   0.0000  0.0000   9.5538  0.0010   0.0000   \n",
       "5666  346.990957  54.326940   0.0000  0.0000   9.8405  0.0010   9.2025   \n",
       "5667  331.173131  46.427253  10.4915  0.0630  10.2538  0.0239  10.0057   \n",
       "5668  321.694044  43.812346   0.0000  0.0000   7.8477  0.3777   0.0000   \n",
       "\n",
       "          7        8       9   ...     16     17     18     19      20  \\\n",
       "0     0.0000   6.0720  0.0000  ...  0.162  0.050  0.015  0.044   5.562   \n",
       "1     0.0000   7.5167  0.0010  ...  0.059  0.020  0.016  0.059   6.896   \n",
       "2     0.0000  10.9890  0.0000  ...  0.022  0.020  0.074  0.000  10.408   \n",
       "3     0.1042   0.0000  0.0000  ...  0.149  0.046  0.015  0.037   5.671   \n",
       "4     0.0000   5.9504  0.0031  ...  0.169  0.072  0.015  0.036   5.430   \n",
       "...      ...      ...     ...  ...    ...    ...    ...    ...     ...   \n",
       "5664  0.0000   9.0066  0.0010  ...  0.022  0.019  0.021  0.144   8.358   \n",
       "5665  0.0000  10.7253  0.0010  ...  0.023  0.019  0.026  0.297   8.778   \n",
       "5666  0.0010  10.1089  0.0374  ...  0.024  0.020  0.030  0.327   9.180   \n",
       "5667  0.0074  10.4418  0.0315  ...  0.023  0.020  0.033  0.000   9.693   \n",
       "5668  0.0000   8.1665  0.0010  ...  0.029  0.020  0.017  0.115   7.248   \n",
       "\n",
       "          21      22     23     24     25  \n",
       "0      5.539   5.479  0.017  0.026  0.020  \n",
       "1      6.921   6.932  0.026  0.024  0.016  \n",
       "2     10.400  10.378  0.026  0.021  0.023  \n",
       "3      5.717   5.704  0.023  0.063  0.020  \n",
       "4      5.402   5.333  0.054  0.036  0.017  \n",
       "...      ...     ...    ...    ...    ...  \n",
       "5664   8.389   8.398  0.027  0.017  0.016  \n",
       "5665   8.804   8.766  0.021  0.016  0.020  \n",
       "5666   9.187   9.180  0.021  0.020  0.022  \n",
       "5667   9.678   9.622  0.024  0.029  0.023  \n",
       "5668   7.309   7.281  0.023  0.042  0.020  \n",
       "\n",
       "[5669 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the data from the mat file\n",
    "with h5py.File(mat_file, 'r') as f:\n",
    "    data = f[\"data\"]\n",
    "    data = pd.DataFrame(data).T\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing 0s with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "columns_with_zero = data.columns[(data == 0).any()].tolist()\n",
    "\n",
    "print(columns_with_zero)\n",
    "target_columns = columns_with_zero\n",
    "\n",
    "for target_column in target_columns:\n",
    "    df_zeros = data[data[target_column] == 0]\n",
    "    df_no_zeros = data[data[target_column] != 0]\n",
    "\n",
    "    X_train = df_no_zeros.drop(columns=target_column)\n",
    "    y_train = df_no_zeros[target_column]\n",
    "\n",
    "    X_test = df_zeros.drop(columns=target_column)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    # Replace the zero values in the original dataframe\n",
    "    data.loc[data[target_column] == 0, target_column] = y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating data into folds, and folds into training and test data, before scaling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "for i in range(len(idTR)):\n",
    "    X_train_list.append(data.iloc[idTR[i] - 1]) #idTR is 1 indexed\n",
    "    X_test_list.append(data.iloc[idTE[i] - 1])  #idTE is 1 indexed\n",
    "    y_train_list.append(labelTR[i])\n",
    "    y_test_list.append(labelTE[i])\n",
    "    #Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_list[i] = scaler.fit_transform(X_train_list[i])\n",
    "    X_test_list[i] = scaler.transform(X_test_list[i])\n",
    "\n",
    "X_train_arr = np.array(X_train_list)\n",
    "X_test_arr = np.array(X_test_list)\n",
    "y_train_arr = np.array(y_train_list)\n",
    "y_test_arr = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4535, 26), (5, 1134, 26), (5, 4535), (5, 1134))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape, X_test_arr.shape, y_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network structure: $26$ x $10$ x $5$ x $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays or pandas DataFrames to PyTorch tensors if needed\n",
    "X_train_tens = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "y_train_tens = torch.tensor(y_train_arr, dtype=torch.long)\n",
    "\n",
    "# Defining a simple neural network class for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, activation):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.activation = activation       \n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)# Output layer with single neuron (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Apply sigmoid activation for binary classification of final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED) # Adding seed for reproducibility for all the activation functions\n",
    "act_losses = [] \n",
    "activation_functions = [nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Hardtanh()]\n",
    "act_colors = {\"ReLU\": \"red\", \"Sigmoid\": \"blue\", \"Tanh\": \"green\", \"Hardtanh\": \"orange\"}\n",
    "\n",
    "for act in activation_functions:\n",
    "    losses = []\n",
    "    for i in range(len(X_train_tens)):\n",
    "        # Initializing\n",
    "        input_size = X_train_tens[i].shape[1]\n",
    "        model = NeuralNet(input_size, act)\n",
    "    \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # Prepare data for training using DataLoader\n",
    "        batch_size = 64\n",
    "        train_dataset = TensorDataset(X_train_tens[i], y_train_tens[i])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training the model\n",
    "        num_epochs = 15\n",
    "        temp_loss = []\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze(dim=1)  # Remove extra dimension for binary classification\n",
    "                loss = criterion(outputs, labels.float())  \n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "            temp_loss.append(running_loss / len(train_loader))\n",
    "        losses.append(temp_loss)\n",
    "\n",
    "        model.eval()  #Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_train_tens[i])\n",
    "            predictions = (predictions > 0.5).int()  # Convert probabilities to binary predictions (0 or 1)\n",
    "\n",
    "\n",
    "        model_dir = f'./models/activation_test/{type(act).__name__}/'\n",
    "        # Checking if the directory exists\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        # Saving the model for later use\n",
    "        model_path = os.path.join(model_dir, f'fold{i}_binary_classification_model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    act_losses.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss on $\\bold{training}$ data plotted over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "act_mean_losses = np.mean(act_losses, axis=1) # Mean loss for each activation function\n",
    "\n",
    "# Plotting each model\n",
    "for i, loss in enumerate(act_mean_losses):\n",
    "    name_act = type(activation_functions[i]).__name__\n",
    "    c = act_colors[name_act]\n",
    "    plt.plot(loss, \"-o\", label=name_act, color = c)\n",
    "plt.legend(fontsize = font_size-2)\n",
    "plt.xlabel(\"Epoch\", size = font_size)\n",
    "plt.ylabel(\"Loss\", size = font_size)\n",
    "plt.title(\"Training Loss vs. Epoch for Different Activation Functions\", size = font_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "for act in activation_functions:\n",
    "    print(f\"Activation function: {type(act).__name__}\")\n",
    "    for i in range(len(X_train_tens)):\n",
    "        print(f\"Evaluating model on fold: {i}\")\n",
    "        # Load the trained model\n",
    "        input_size = input_size  \n",
    "        model = NeuralNet(input_size, act)\n",
    "        model.load_state_dict(torch.load(f'models/activation_test/{type(act).__name__}/fold{i}_binary_classification_model.pth')) \n",
    "\n",
    "        X_train = X_train_tens[i].float()\n",
    "        y_train = y_train_tens[i].float()\n",
    "\n",
    "        #Prepare test dataset and dataloader\n",
    "        batch_size = 64\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)  #No need to shuffle for testing\n",
    "\n",
    "        model.eval()  #Switch to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad(): #No need for gradient on test run\n",
    "            for inputs, labels in train_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate Matthews Correlation Coefficient (MCC)\n",
    "        mcc_tr = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "        print(f\"Training: MCC-score: {mcc_tr}, check against the test set for overfitting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_all_metrics = {}\n",
    "act_mean_metrics = {}\n",
    "act_all_confusion_matrices = {}\n",
    "act_mean_confusion_matrices = {}\n",
    "\n",
    "for act in activation_functions:\n",
    "    all_metrics = {}\n",
    "    all_confusion_matrices = {}\n",
    "    act_name = type(act).__name__\n",
    "    print(f\"Activation function: {act_name}\")\n",
    "    print(\"---------------------------------\")\n",
    "    temp_F1, temp_MCC, temp_acc, temp_prec, temp_rec = 0,0,0,0,0\n",
    "    for i in range(len(X_train_tens)):\n",
    "        print(f\"    Evaluating model on fold: {i}\")\n",
    "        # Loading the previously trained model\n",
    "        input_size = input_size \n",
    "        model = NeuralNet(input_size, act)\n",
    "        model.load_state_dict(torch.load(f'models/activation_test/{act_name}/fold{i}_binary_classification_model.pth'))\n",
    "\n",
    "        X_test = torch.tensor(X_test_arr[i], dtype=torch.float32)\n",
    "        y_test = torch.tensor(y_test_arr[i], dtype=torch.float32)\n",
    "\n",
    "        # Prepare test dataset and dataloader\n",
    "        batch_size = 64\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  #No need to shuffle for testing\n",
    "\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad(): # No need to calculate gradients in test run\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculating different scores\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred) #Calculate Matthews Correlation Coefficient (MCC)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='binary') #Binary average, since we have a binary classification problem\n",
    "        recall = recall_score(y_true, y_pred, average='binary')  \n",
    "        conf_matrix = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "\n",
    "        #Stores the metrics in a dictionary for easy plotting\n",
    "        metrics = {'F1 Score': f1, 'MCC': mcc, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "        \n",
    "        # Saving values to calculate averages\n",
    "        temp_F1 += f1\n",
    "        temp_acc += accuracy\n",
    "        temp_prec += precision\n",
    "        temp_rec += recall\n",
    "        temp_MCC += mcc\n",
    "\n",
    "        all_metrics[f\"fold_{i}\"] = metrics\n",
    "        all_confusion_matrices[f\"fold_{i}\"] = conf_matrix\n",
    "        print(f\"Testing MCC score: {metrics['MCC']}\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"\\n\")\n",
    "    #Saving the metrics and confusion matrices with respective act. func. for later use\n",
    "    act_all_metrics[act_name] = all_metrics\n",
    "    act_all_confusion_matrices[act_name] = all_confusion_matrices\n",
    "    act_mean_metrics[act_name] = {'F1 Score': temp_F1/len(X_train_tens), 'MCC': temp_MCC/len(X_train_tens), 'Accuracy': temp_acc/len(X_train_tens), 'Precision': temp_prec/len(X_train_tens), 'Recall': temp_rec/len(X_train_tens)}\n",
    "    act_mean_confusion_matrices[act_name] = temp_MCC/len(X_train_tens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_metric = pd.DataFrame(act_mean_metrics)\n",
    "data_metric = data_metric.reset_index().rename(columns={'index': 'Metrics'})\n",
    "\n",
    "\n",
    "data_metric = pd.melt(data_metric, id_vars='Metrics', var_name='Activation Function', value_name='Value')\n",
    "\n",
    "data_metric = data_metric.set_index(['Metrics', 'Activation Function']).Value\n",
    "colors = [\"orange\", \"red\", \"blue\", \"green\"]\n",
    "data_metric.unstack().plot(kind='bar', stacked=False, color = colors, fontsize = font_size-3)\n",
    "plt.legend(fontsize = font_size-2, loc = 'lower center')\n",
    "plt.ylim(0.8,1)\n",
    "plt.title(\"Average Metrics for Different Activation Functions\", size = header_font_size-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mean_confusion_matrices\n",
    "# act_all_confusion_matrices\n",
    "act_all_confusion_matrices\n",
    "\n",
    "mean_confusion_matrices = {}\n",
    "for act_func, matrices in act_all_confusion_matrices.items():\n",
    "    stacked_matrices = np.dstack(list(matrices.values()))\n",
    "    mean_matrix = np.mean(stacked_matrices, axis=2, dtype=int)\n",
    "    mean_confusion_matrices[act_func] = mean_matrix\n",
    "\n",
    "# print(mean_confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, len(activation_functions), figsize=(20, 5))\n",
    "\n",
    "for i, act in enumerate(activation_functions):\n",
    "    act_name = type(act).__name__\n",
    "    conf_matrix = mean_confusion_matrices[act_name]\n",
    "\n",
    "    #Creates a heatmap for the confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar=False, annot_kws={\"size\": font_size})\n",
    "\n",
    "    axes[i].set_title(f'{act_name}', size = header_font_size)\n",
    "    axes[i].set_xlabel('Predicted', size = font_size)\n",
    "    axes[i].set_ylabel('True', size = font_size)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "fig.suptitle(\"Average Confusion Matrices for Different Activation Functions\", fontsize=header_font_size + 5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
